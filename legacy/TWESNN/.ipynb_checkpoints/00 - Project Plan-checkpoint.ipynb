{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why use Neural Networks?**\n",
    "> <br>\n",
    "> Neural Networks are more efficient that other machine learning techniques for identification of complex patterns in data.\n",
    "\n",
    "## Learning Classes\n",
    "* Unsupervised - detect existing patterns in data\n",
    "* Supervised - training set of labelled data used to predict future data\n",
    "* Semi-supervised - mix of both labelled and unlabelled data available\n",
    "* Reinforcement - environmental interaction of reward and penalty feedback used to learn\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Network = Nodes + Edges\n",
    "\n",
    "Neural Network = Neurons + Synapses\n",
    "\n",
    "Each Edge has a weight\n",
    "<br>\n",
    "Each Node has a bias\n",
    "\n",
    "To define a project we need:\n",
    "* Neuron Type\n",
    "* Network Size\n",
    "* Network Topology\n",
    "* Input/Output Data Encoding/Decoding Method\n",
    "* Training Method\n",
    "* Initial Weights and Bias\n",
    "\n",
    "## Neural Network Learning Theories\n",
    "* Hebbian Law - any two cells or systems of cells that are repeatedly active at the same time will tend to become 'associated', so that activity in one facilitates activity in the other\n",
    "* BCM theory\n",
    "* Oja's rule\n",
    "* Generalized Hebbian Algorithm\n",
    "\n",
    "## 3 Generation of Neurons:\n",
    "1. McCullochâ€“Pitts Neuron Model (step threshold)\n",
    "2. Activation Function\n",
    "3. Spiking Neurons\n",
    "    * Hodgkin-Huxley Model - most biologically real\n",
    "    * Izhikevich Model\n",
    "    * Spike Response Model\n",
    "    * Leaky Integrate-and-Fire Model - computationally efficient\n",
    "    * [etc..](https://ieeexplore.ieee.org/document/1333071/)\n",
    "\n",
    "Gen1/Gen2 are commonly called Artificial Neural Networks (ANN).\n",
    "<br>\n",
    "Gen3, though also a model of ANN, are specifically referred to as Spiking Neural Networks (SNN)\n",
    "\n",
    "#### ANN vs. SNN\n",
    "###### Neutral\n",
    "* ANN are a abstracted simplification of the biological process. SNN tries to model the biological process more closely.\n",
    "* Both ANN and SNN can be Turing Complete with inclusion of Memory Cells and can perform universal computation.\n",
    "\n",
    "###### ANN Pros\n",
    "* ANN are static and differentiable, thus can be trained using backpropagation. SNN are dynamic and requires different training techniques.\n",
    "* ANN are computationally easy to implement. SNN requires more complex computations for the biological process replication using differential equations.\n",
    "* ANN can achieve high efficiency and large network size with current hardwares. SNN requires specialised hardwares for efficient implementation (like Neuromorphic Processors), else gives lower efficiency or limited sizes.\n",
    "\n",
    "###### SNN Pros\n",
    "* ANN cannot encode the temporal data of the input. SNN are good in encoding the temporal model allowing an extra dimension for information encoding, leading to reduction in number of neurons for identical tasks.\n",
    "\n",
    "> **Which neural model to use?**\n",
    "> <br>\n",
    "> (Given current development of the field) SNN are suited for biological process modelling. ANN is recommended for end-use machine learning implementations.\n",
    "\n",
    "## Training Methods\n",
    "Training is the process of adjusting the weights and biases for the network\n",
    "* Backpropagation with Gradient Descent\n",
    "* Competitive Learning\n",
    "* Adversarial Training\n",
    "* Spike Time Dependent Plasticity\n",
    "* Remote Supervised Method\n",
    "\n",
    "## Relevant Network Topologies\n",
    "* Markov Chain (MC)\n",
    "* Support Vector Machine (SVM) - Binary classification.\n",
    "* Convolution Neural Network (CNN) - Useful for object recognition in images. Has layers like Conv, ReLU, Pooling and FC.\n",
    "* Recursive Neural Tensor Network (RNTN) - Useful for hierarchial structured data like Natural Language Processing, sentiment analysis, video scenes.\n",
    "* Kohonen Network (KN) - Also called Self-Organising Feature Map (SOFM), used for unsupervised dimensionality reduction. It used competitive learning. It has a feed-foward toroidal grid structure with emergent properties.\n",
    "* Recurrent Neural Network (RNN) - Single layer where output is fed back to input in next time step. Useful for time series data in speech, stocks, suppy chain, etc.\n",
    "    * Long Short Term Memory (LSTM)\n",
    "    * Gated Recurrent Unit (GRU)\n",
    "* Bidirectional Associative Memory (BAM) - Hetero-associative memory using 2 layers of Recurrent Neural Network.\n",
    "* Hopfield Network - Auto-associative memory using Recurrent Neural Network.\n",
    "* Boltzmann Machine - Fully connected network. Also called stochastic generative Hopfield network with hidden units. Energy Based Model based on Boltzmann distribution.\n",
    "* Auto Encoder (AE) - Unsupervised feature extractor by reconstructing/generating learnt patterns. Useful for dimensionality reduction.\n",
    "    * Variational Auto Encoder (VAE)\n",
    "    * Denoising Auto Encoder (DAE)\n",
    "    * Sparse Auto Encoder (SAE)\n",
    "* Restricted Boltzmann Machine (RBM) - A type of Auto Encoder for a bipartite variant of Boltzmann Machine.\n",
    "* Reservoir Computing - It is an extension of Neural Network in a dynamic model in temporal domain learning.\n",
    "    * Liquid State Machines (LSM) - A type of Spiking Recurrent Neural Network.\n",
    "    * Echo State Networks (ESN) - A type of Recurrent Neural Network.\n",
    "* Extreme Learning Machine (ELM) - Sparsely connected feedforward network.\n",
    "* Generative Adversarial Network (GAN) - Double networks, that are composed from generator and discriminator trying to fool each other.\n",
    "* Cortical Algorithm (CA) - Also called Hierarchical Temporal Memory (HTM). Learns time-based patterns in unlabeled data on a continuous basis.\n",
    "* Deep Neural Networks (DNN) - Allows learning more complex associations in each hidden layer, compared to shallow networks. Better models complex neural hierarchy in biological brain bu requires high computation power. Difficult to train using back propagation due to vanishing gradient problem.\n",
    "    * Deep Convolution Network\n",
    "    * Deep Echo State Network - Hierarchical temporal feature representation at different levels of abstraction.\n",
    "    * Deep Belief Network (DBN) - Made out of stack of Restricted Boltzmann Machines and Variational Auto Encoders. They are pre-trained layerwise using unsupervised KL divergence. Useful for supevised learning for recognition or generation.\n",
    "    * Deep Auto Encoder (DAE) - Outperforms Principle Component Analysis in dimensionality reduction. Useful for unsupervised feature extraction. Made of 2 Deep Belief Networks connected for encoding and decoding.\n",
    "\n",
    "> **Which neural topology to use?**\n",
    "> <br>\n",
    "> Depends on the application. For learning complex pattern association, Deep Belief Network is a good choice.\n",
    "\n",
    "## Implementation Tools\n",
    "Platforms provide out-of-the-box implementation without much coding, while Software Libraries allows more control over network selection and hyperparameter configuration using predefined modules.\n",
    "##### Platforms\n",
    "* ErSatz\n",
    "* H2O\n",
    "* Dat Graphlab\n",
    "* etc..\n",
    "\n",
    "##### Software Libraries\n",
    "* TensorFlow\n",
    "* Theano\n",
    "* Torch\n",
    "* Caffe\n",
    "* DeepMat\n",
    "* DeepLearning4j\n",
    "* Keras\n",
    "* etc..\n",
    "\n",
    "##### Neural Modelling\n",
    "* [Brain2](https://brian2.readthedocs.io/en/stable/index.html)\n",
    "* [NEST](http://www.nest-simulator.org/)\n",
    "* [Nengo](https://www.nengo.ai/)\n",
    "* [Neuroph](http://neuroph.sourceforge.net/)\n",
    "* [Encog](https://www.heatonresearch.com/encog/)\n",
    "* Neural\n",
    "* etc..\n",
    "\n",
    "> **Which tool to use?**\n",
    "> <br>\n",
    "> TensorFlow (with Keras) gives a good mix of control and programming ease for Neural Network based machine learning. \n",
    "\n",
    "## Topology and Weight Evolving Artificial Neural Network (TWEANN)\n",
    "\n",
    "NeuroEvolution of Augmented Topologies (NEAT)\n",
    "___\n",
    "\n",
    "# <center> PLAN </center>\n",
    "# <center> Topology and Weight Evolving Spiking Neural Network </center>\n",
    "\n",
    "### Phase 1 : TensorFlow & Neural Networks\n",
    "Step 1.1: Familiarizing with TensorFlow\n",
    "<br>\n",
    "Step 1.2: Restricted Boltzmann Machine for MNIST Dataset in TensorFlow\n",
    "<br>\n",
    "Step 1.3: [Deep Belief Network for MNIST Dataset in TensorFlow](https://cognitiveclass.ai/courses/deep-learning-tensorflow/)\n",
    "### Phase 2 : NeuroEvolution\n",
    "Step 2.1: [NeuroEvolution of Augmenting Topologies for XOR](https://github.com/oxalorg/neat.py) in TensorFlow\n",
    "<br>\n",
    "Step 2.2: NeuroEvolution of Augmenting Topologies for MNIST in TensorFlow\n",
    "<br>\n",
    "Step 2.3: Iterated Evolvable-substrate-HyperNEAT for MNIST Dataset in TensorFlow\n",
    "### Phase 3 : Spiking Neurons\n",
    "Step 3.1: [Leaky Integrate and Fire Spiking Neural Network for Binary Classification in TensorFlow](https://github.com/kaizouman/tensorsandbox/tree/master/snn)\n",
    "<br>\n",
    "Step 3.2: Leaky Integrate and Fire Spiking Neural Network for MNIST Dataset in TensorFlow\n",
    "<br>\n",
    "Step 3.3: Izhikevich Model Spiking Neural Network for MNIST Dataset in TensorFlow\n",
    "### Phase 4 : Integrate\n",
    "Step 4.1: IES-HyperNEAT on LIF SNN for MNIST Dataset in TensorFlow\n",
    "<br>\n",
    "Step 4.2: IES-HyperNEAT on LIF SNN for Multi-Associative Memory in TensorFlow\n",
    "<br>\n",
    "Step 4.3: IES-HyperNEAT on HH SNN for MAM in TensorFlow\n",
    "### Phase 5: Neuromorphic Hardware\n",
    "Step 5.1: SNN on Neuromorphic Chip\n",
    "<br>\n",
    "Step 5.2: [NEAT on LIF SNN for XOR on NMC](https://www.researchgate.net/publication/313804390_Neuro-evolution_of_spiking_neural_networks_on_SpiNNaker_neuromorphic_hardware)\n",
    "<br>\n",
    "Step 5.3: **IES-HyperNEAT on LIF SNN for MAM on NMC**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
