{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Phase 1: TensorFlow for Neural Networks </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download util library for weight plotting\n",
    "import urllib.request\n",
    "with urllib.request.urlopen(\"http://deeplearning.net/tutorial/code/utils.py\") as url:\n",
    "    response = url.read()\n",
    "target = open('utils.py', 'w')\n",
    "target.write(response.decode('utf-8'))\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from utils import tile_raster_images\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST data. It has 55000 training data and 10000 test data for labeled handwritten images of 28\\*28 pixels flattened to 1-D. The labels are in one-hot boolean array encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "# print(trX.shape,trY.shape,teX.shape,teY.shape)\n",
    "# img = Image.fromarray(trX[3].reshape(28,28)*255)\n",
    "# imgplot = plt.imshow(img)\n",
    "# print(trY[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Restricted Boltzmann Machine </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricted Boltzmann Machine (RBM) are shallow neural nets that learn to reconstruct data by themselves in an unsupervised fashion.\n",
    "\n",
    "RBM takes the inputs and translates them to a set of numbers that represents them. Then, these numbers can be translated back to reconstruct the inputs. Through several forward and backward passes, the RBM will be trained, and a trained RBM can reveal which features are the most important ones when detecting patterns.   \n",
    "\n",
    "**Applications of RBM**\n",
    "<br>\n",
    "RBM is useful for collaborative filtering, dimensionality reduction, classification, regression, feature learning, topic modeling and Deep Belief Networks.\n",
    "\n",
    "**RBM - a generative model**\n",
    "<br>\n",
    "_Discriminative:_ Given a training set, a discriminative model tries to find a decision boundary (in n-dimension) that separates the classes. \n",
    "<br>\n",
    "_Generative:_ To classify the test data is matched against each trained models to specify a probability distribution over a dataset of input vectors. We can do both supervised and unsupervised tasks with generative models:\n",
    "- In an unsupervised task, we try to form a model for P(x), where x is an input vector. \n",
    "- In the supervised task, we first form a model for P(x|y), where y is the label for x. For example, if y indicates whether an example is a SUV (0) or a Sedan (1), then p(x|y = 0) models the distribution of SUVs’ features, and p(x|y = 1) models the distribution of Sedans’ features. If we manage to find P(x|y) and P(y), then we can use `bayes rule` to estimate P(y|x), because: p(y|x) = p(x|y)p(y)/p(x)\n",
    "\n",
    "### RBM Topology\n",
    "RBM has two layers.\n",
    "- The first layer of the RBM is called the visible (or input layer). MNIST images have 784 pixels, so the visible layer must have _n=784_ input nodes. \n",
    "- The second layer is the hidden layer, which possesses m neurons in our case. Each hidden unit has a binary state, which we’ll call sm, and turns either on or off (i.e., sm = 1 or sm = 0) with a probability that is a logistic function of the inputs it receives from the other n visible units, called for example, p(sm = 1). For our case, we'll use *m=500* nodes in the hidden layer.\n",
    "- Each node also has a bias. We denote the bias as “vb” for the visible units and “hb” for the hidden units.\n",
    "- Weights among the input layer and hidden layer nodes are defined in the weight matrix. The rows are equal to the input nodes, and the columns are equal to the output nodes.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*e91JOYKp6Vh69pw_YNVSlQ.png\" alt=\"RBM Model\" style=\"width: 300px;\"/>\n",
    "\n",
    "### RBM Training\n",
    "<br>\n",
    "RBM has two phases:\n",
    "\n",
    "__1. Forward pass:__  Processing happens in each node in the hidden layer. That is, input data from all visible nodes are being passed to all hidden nodes. This computation begins by making stochastic decisions about whether to transmit that input or not (i.e. to determine the state of each hidden layer). At the hidden layer's nodes, X is multiplied by W and added to h\\_bias. The result of those two operations is fed into the sigmoid function, which produces the node’s output/state. As a result, one output is produced for each hidden node. So, for each row in the training set, a tensor of probabilities is generated, which in our case it is of size [1x500], and totally 55000 vectors (_h0_=[55000x500]).\n",
    "\n",
    "Then, we take the tensor of probabilities (as from a sigmoidal activation) and make samples from all the distributions, h0.  That is, we sample the activation vector from the probability distribution of hidden layer values. Samples are used to estimate the negative phase gradient which will be explained later.\n",
    "\n",
    "__2. Backward Pass (Reconstruction):__\n",
    "The RBM reconstructs data by making several forward and backward passes between the visible and hidden layers.\n",
    "\n",
    "So, in the second phase (i.e. reconstruction phase), the samples from the hidden layer (i.e. h0) play the role of input. That is, h0 becomes the input in the backward pass. The same weight matrix and visible layer biases are used to go through the sigmoid function. The produced output is a reconstruction which is an approximation of the original input.\n",
    "\n",
    "**Weight update**\n",
    "\n",
    "In order to train an RBM, we have to maximize the product of probabilities assigned to the training set V (a matrix, where each row of it is treated as a visible vector v):\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d42e9f5aad5e1a62b11b119c9315236383c1864a\" >\n",
    "\n",
    "Or equivalently, maximize the expected log probability of V:\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ba0ceed99dca5ff1d21e5ace23f5f2223f19efc0\" >\n",
    "\n",
    "Equivalently, we can define the objective function as __the average negative log-likelihood__ and try to minimize it. To achieve this, we need the partial derivative of this function in respect to all of its parameters. And it can be shown that the above equation is indirectly the weights and biases function, so minimization of the objective function here means modifying/optimizing the weight vector W. So, we can use __stochastic gradient descent__ to find the optimal weight and consequently minimize the objective function. When we derive, it give us 2 terms, called positive and negative gradient. These negative and positive phases reflect their effect on the probability density defined by the model. The positive one depends on observations (X), and the second one depends on only the model. \n",
    " \n",
    "The __Positive phase__ increases the probability of training data.  \n",
    "The __Negative phase__ decreases the probability of samples generated by the model.  \n",
    "\n",
    "The negative phase is hard to compute, so we use a method called __Contrastive Divergence (CD)__ to approximate it.  It is designed in such a way that at least the direction of the gradient estimate is somewhat accurate, even when the size is not (In real world models, more accurate techniques like CD-k or PCD are used to train RBMs). During the calculation of CD, we have to use __Gibbs sampling__ to sample from our model distribution.    \n",
    "\n",
    "Contrastive Divergence is actually matrix of values that is computed and used to adjust values of the W matrix. Changing W incrementally leads to training of W values. Then on each step (epoch), W is updated to a new value W' through the equation below:\n",
    "$W' = W + alpha * CD$ \n",
    "\n",
    "__Alpha__ is some small step rate and is also known as the \"learning rate\".\n",
    "\n",
    "1. Take a training sample from X, compute the probabilities of the hidden units and sample a hidden activation vector h0 from this probability distribution.\n",
    " - $\\_h0 = sigmoid(X \\otimes W + hb)$\n",
    " - $h0 = sampleProb(h0)$\n",
    "2. Compute the outer product of X and h0 and call this the positive gradient.\n",
    " - $w\\_pos\\_grad = X \\otimes h0$  (Reconstruction in the first pass)  \n",
    "3. From h, reconstruct v1, and then take a sample of the visible units, then resample the hidden activations h1 from this. (Gibbs sampling step)\n",
    " - $\\_v1 = sigmoid(h0 \\otimes transpose(W) + vb)$\n",
    " - $v1 = sampleProb(v1)$  (Sample v given h)\n",
    " - $\\_h1 = sigmoid(v1 \\otimes W + hb)$\n",
    "4. Compute the outer product of v1 and \\_h1 and call this the negative gradient.\n",
    " - $w\\_neg\\_grad = v1 \\otimes \\_h1$  (Reconstruction 1)\n",
    "5. Now, CD equals the positive gradient minus the - negative gradient, CD is a matrix of size 784x500. \n",
    " - $CD = (w\\_pos\\_grad - w\\_neg\\_grad) / datapoints$\n",
    "6. Update the weight to be CD times some learning rate\n",
    " - $W' = W + alpha*CD$\n",
    "7. At the end of the algorithm, the visible nodes will store the value of the sample.\n",
    "\n",
    "**Objective function**\n",
    "\n",
    "__Calculate error:__  \n",
    "In each epoch, we compute the \"error\" as a sum of the squared difference between step 1 and step n,\n",
    "e.g the error shows the difference between the data and its reconstruction.\n",
    "\n",
    "__Note:__ tf.reduce_mean computes the mean of elements across dimensions of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that defines the behavior of the RBM\n",
    "class RBM(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        #Defining the hyperparameters\n",
    "        self._input_size = input_size #Size of input\n",
    "        self._output_size = output_size #Size of output\n",
    "        self.epochs = 5 #Amount of training iterations\n",
    "        self.learning_rate = 1.0 #The step used in gradient descent\n",
    "        self.batchsize = 100 #The size of how much data will be used for training per sub iteration\n",
    "        \n",
    "        #Initializing weights and biases as matrices full of zeroes\n",
    "        self.w = np.zeros([input_size, output_size], np.float32) #Creates and initializes the weights with 0\n",
    "        self.hb = np.zeros([output_size], np.float32) #Creates and initializes the hidden biases with 0\n",
    "        self.vb = np.zeros([input_size], np.float32) #Creates and initializes the visible biases with 0\n",
    "\n",
    "\n",
    "    #Fits the result from the weighted visible layer plus the bias into a sigmoid curve\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        #Sigmoid \n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    #Fits the result from the weighted hidden layer plus the bias into a sigmoid curve\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    #Generate the sample probability\n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "    #Training method for the model\n",
    "    def train(self, X):\n",
    "        #Create the placeholders for our parameters\n",
    "        _w = tf.placeholder(\"float\", [self._input_size, self._output_size])\n",
    "        _hb = tf.placeholder(\"float\", [self._output_size])\n",
    "        _vb = tf.placeholder(\"float\", [self._input_size])\n",
    "        \n",
    "        prv_w = np.zeros([self._input_size, self._output_size], np.float32) #Creates and initializes the weights with 0\n",
    "        prv_hb = np.zeros([self._output_size], np.float32) #Creates and initializes the hidden biases with 0\n",
    "        prv_vb = np.zeros([self._input_size], np.float32) #Creates and initializes the visible biases with 0\n",
    "\n",
    "        cur_w = np.zeros([self._input_size, self._output_size], np.float32)\n",
    "        cur_hb = np.zeros([self._output_size], np.float32)\n",
    "        cur_vb = np.zeros([self._input_size], np.float32)\n",
    "        \n",
    "        v0 = tf.placeholder(\"float\", [None, self._input_size])\n",
    "        \n",
    "        #Initialize with sample probabilities\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        #Create the Gradients\n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "        \n",
    "        #Update learning rates for the layers\n",
    "        update_w = _w + self.learning_rate *(positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "        update_vb = _vb +  self.learning_rate * tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        #Find the error rate\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        #Training loop\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #For each epoch\n",
    "            for epoch in range(self.epochs):\n",
    "                #For each step/batch\n",
    "                for start, end in zip(range(0, len(X), self.batchsize),range(self.batchsize,len(X), self.batchsize)):\n",
    "                    batch = X[start:end]\n",
    "                    #Update the rates\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "\n",
    "    #Create expected output for our DBN\n",
    "    def rbm_outpt(self, X):\n",
    "        input_X = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        out = tf.nn.sigmoid(tf.matmul(input_X, _w) + _hb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Deep Belief Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with traditional multilayer perceptrons/artificial neural networks is that backpropagation can often lead to “local minima”. This is when your “error surface” contains multiple grooves and you fall into a groove that is not lowest possible groove as you perform gradient descent.\n",
    "\n",
    "__Deep belief networks__ solve this problem by using an extra step called __pre-training__. Pre-training is done before backpropagation and can lead to an error rate not far from optimal. This puts us in the “neighborhood” of the final solution. Then backpropagation slowly reduces the error rate from there.\n",
    "\n",
    "DBNs can be divided in two major parts. The first one are multiple layers of Restricted Boltzmann Machines (RBMs) to pre-train our network. The second one is a feed-forward backpropagation network, that will further refine the results from the RBM stack.\n",
    "\n",
    "<img src=\"https://www.ibm.com/developerworks/library/cc-machine-learning-deep-learning-architectures/figure07.png\" alt=\"DBN Model\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DBN with 3 RBMs, one with 500 hidden units, the second one with 200 and the last one with 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM_hidden_sizes = [500, 200 , 50 ] #create 2 layers of RBM with size 400 and 100\n",
    "\n",
    "#Since we are training, set input as training data\n",
    "inpX = trX\n",
    "\n",
    "#Create list to hold our RBMs\n",
    "rbm_list = []\n",
    "\n",
    "#Size of inputs is the number of inputs in the training set\n",
    "input_size = inpX.shape[1]\n",
    "#For each RBM we want to generate\n",
    "for i, size in enumerate(RBM_hidden_sizes):\n",
    "    print ('RBM: ',i,' ',input_size,'->', size)\n",
    "    rbm_list.append(RBM(input_size, size))\n",
    "    input_size = size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train the DBN (train each RBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each RBM in our list\n",
    "for rbm in rbm_list:\n",
    "    print ('New RBM:')\n",
    "    #Train a new one\n",
    "    rbm.train(inpX) \n",
    "    #Return the output layer\n",
    "    inpX = rbm.rbm_outpt(inpX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize trained weight matrix for each hidden node.\n",
    "wtplt = rbm_list[0].w.T\n",
    "tile_raster_images(X=wtplt, img_shape=(28, 28), tile_shape=(25, 20), tile_spacing=(1, 1))\n",
    "image = Image.fromarray(tile_raster_images(X=wtplt, img_shape=(28, 28) ,tile_shape=(25, 20), tile_spacing=(1, 1)))\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the RBM (section under edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_case = teX[1:2]\n",
    "img = Image.fromarray(tile_raster_images(X=sample_case, img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n",
    "plt.rcParams['figure.figsize'] = (2.0, 2.0)\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.set_cmap('gray')  #you can experiment different colormaps (Greys,winter,autumn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the RBM on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh0 = tf.nn.sigmoid(tf.matmul(X, W) + hb)\n",
    "vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "feed = sess.run(hh0, feed_dict={ X: sample_case, W: prv_w, hb: prv_hb})\n",
    "rec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(tile_raster_images(X=rec, img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n",
    "plt.rcParams['figure.figsize'] = (2.0, 2.0)\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.set_cmap('gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised training\n",
    "\n",
    "Convert the learned representation of input data into a supervised prediction, e.g. a linear classifier. The output of the last hidden layer of the DBN is used to classify digits using a shallow Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, sizes, X, Y):\n",
    "        #Initialize hyperparameters\n",
    "        self._sizes = sizes\n",
    "        self._X = X\n",
    "        self._Y = Y\n",
    "        self.w_list = []\n",
    "        self.b_list = []\n",
    "        self._learning_rate =  1.0\n",
    "        self._momentum = 0.0\n",
    "        self._epoches = 12\n",
    "        self._batchsize = 100\n",
    "        input_size = X.shape[1]\n",
    "        \n",
    "        #initialization loop\n",
    "        for size in self._sizes + [Y.shape[1]]:\n",
    "\n",
    "            #Define upper limit for the uniform distribution range\n",
    "            max_range = 4 * math.sqrt(6. / (input_size + size))\n",
    "            \n",
    "            #Initialize weights through a random uniform distribution\n",
    "            self.w_list.append(np.random.uniform( -max_range, max_range, [input_size, size]).astype(np.float32))\n",
    "            \n",
    "            #Initialize bias as zeroes\n",
    "            self.b_list.append(np.zeros([size], np.float32))\n",
    "            input_size = size\n",
    "            \n",
    "    #load data from rbm\n",
    "    def load_from_rbms(self, dbn_sizes,rbm_list):\n",
    "        #Check if expected sizes are correct\n",
    "        assert len(dbn_sizes) == len(self._sizes)\n",
    "        \n",
    "        for i in range(len(self._sizes)):\n",
    "            #Check if for each RBN the expected sizes are correct\n",
    "            assert dbn_sizes[i] == self._sizes[i]\n",
    "        \n",
    "        #If everything is correct, bring over the weights and biases\n",
    "        for i in range(len(self._sizes)):\n",
    "            self.w_list[i] = rbm_list[i].w\n",
    "            self.b_list[i] = rbm_list[i].hb\n",
    "\n",
    "    #Training method\n",
    "    def train(self):\n",
    "        #Create placeholders for input, weights, biases, output\n",
    "        _a = [None] * (len(self._sizes) + 2)\n",
    "        _w = [None] * (len(self._sizes) + 1)\n",
    "        _b = [None] * (len(self._sizes) + 1)\n",
    "        _a[0] = tf.placeholder(\"float\", [None, self._X.shape[1]])\n",
    "        y = tf.placeholder(\"float\", [None, self._Y.shape[1]])\n",
    "        \n",
    "        #Define variables and activation function\n",
    "        for i in range(len(self._sizes) + 1):\n",
    "            _w[i] = tf.Variable(self.w_list[i])\n",
    "            _b[i] = tf.Variable(self.b_list[i])\n",
    "        for i in range(1, len(self._sizes) + 2):\n",
    "            _a[i] = tf.nn.sigmoid(tf.matmul(_a[i - 1], _w[i - 1]) + _b[i - 1])\n",
    "\n",
    "        #Define the cost function\n",
    "        cost = tf.reduce_mean(tf.square(_a[-1] - y))\n",
    "        \n",
    "        #Define the training operation (Momentum Optimizer minimizing the Cost function)\n",
    "        train_op = tf.train.MomentumOptimizer(self._learning_rate, self._momentum).minimize(cost)\n",
    "        \n",
    "        #Prediction operation\n",
    "        predict_op = tf.argmax(_a[-1], 1)\n",
    "        \n",
    "        #Training Loop\n",
    "        with tf.Session() as sess:\n",
    "            #Initialize Variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #For each epoch\n",
    "            for i in range(self._epoches):\n",
    "                \n",
    "                #For each step\n",
    "                for start, end in zip(range(0, len(self._X), self._batchsize), range(self._batchsize, len(self._X), self._batchsize)):\n",
    "                    #Run the training operation on the input data\n",
    "                    sess.run(train_op, feed_dict={_a[0]: self._X[start:end], y: self._Y[start:end]})\n",
    "                \n",
    "                for j in range(len(self._sizes) + 1):\n",
    "                    #Retrieve weights and biases\n",
    "                    self.w_list[j] = sess.run(_w[j])\n",
    "                    self.b_list[j] = sess.run(_b[j])\n",
    "                \n",
    "                print (\"Accuracy rating for epoch \" + str(i) + \": \" + str(np.mean(np.argmax(self._Y, axis=1) == sess.run(predict_op, feed_dict={_a[0]: self._X, y: self._Y}))))\n",
    "                \n",
    "    #Training method\n",
    "    def pred(self,X):\n",
    "        #Create placeholders for input, weights, biases, output\n",
    "        _a = [None] * (len(self._sizes) + 2)\n",
    "        _w = [None] * (len(self._sizes) + 1)\n",
    "        _b = [None] * (len(self._sizes) + 1)\n",
    "        _a[0] = tf.placeholder(\"float\", [None, self._X.shape[1]])\n",
    "        #y = tf.placeholder(\"float\", [None, self._Y.shape[1]])\n",
    "        \n",
    "        #Define variables and activation function\n",
    "        for i in range(len(self._sizes) + 1):\n",
    "            _w[i] = tf.Variable(self.w_list[i])\n",
    "            _b[i] = tf.Variable(self.b_list[i])\n",
    "        for i in range(1, len(self._sizes) + 2):\n",
    "            _a[i] = tf.nn.sigmoid(tf.matmul(_a[i - 1], _w[i - 1]) + _b[i - 1])\n",
    "\n",
    "        \n",
    "        predict_op = tf.argmax(_a[-1], 1)\n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            opt = sess.run(predict_op, feed_dict={_a[0]: X})\n",
    "        \n",
    "        print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nNet = NN(RBM_hidden_sizes, trX, trY)\n",
    "nNet.load_from_rbms(RBM_hidden_sizes,rbm_list)\n",
    "nNet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataNo = 141 # Between 0-9999\n",
    "\n",
    "nNet.pred(teX[testDataNo:testDataNo+1])\n",
    "\n",
    "sample_case = teX[testDataNo:testDataNo+1]\n",
    "img = Image.fromarray(tile_raster_images(X=sample_case, img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n",
    "plt.rcParams['figure.figsize'] = (2.0, 2.0)\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.set_cmap('gray') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
